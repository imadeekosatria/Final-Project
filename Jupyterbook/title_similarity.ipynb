{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "\n",
    "f = open('PNS Kemendagri Halalbihalal Lebaran 2022 di Metaverse.txt', encoding= 'utf-8')\n",
    "title = 'PNS Kemendagri Halalbihalal Lebaran 2022 di Metaverse'\n",
    "text = f.read()\n",
    "# lower = text.lower()\n",
    "f.close()\n",
    "# print(text)\n",
    "# print(len(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jadikan satu baris teks\n",
    "\n",
    "# print(len(text.split('.')))\n",
    "new_text =  ' '.join(text.splitlines())\n",
    "# print(new_text)\n",
    "\n",
    "# f = open('new_text.txt', 'w')\n",
    "# f.write(new_text)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentasi\n",
    "\n",
    "# import re\n",
    "dict = {}\n",
    "n = 1\n",
    "for i in new_text.split('.'):\n",
    "    for k in i.split('\\n'):\n",
    "        if i == '':\n",
    "            break\n",
    "        # i = re.sub('\\s+', ' ', i)\n",
    "        tes = {'kalimat' : i}\n",
    "        dict['kalimat ' + str(n)] = tes\n",
    "        n += 1\n",
    "# print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filterisasi dan lower case\n",
    "import string\n",
    "import re\n",
    "no = 1\n",
    "for dic in dict:\n",
    "    text_example = dict['kalimat ' + str(no)]['kalimat'].lower()\n",
    "    # print(text_example)\n",
    "    # re_punct = \"\".join([char for char in text_example if char not in string.punctuation])\n",
    "    text = re.sub(r\"\\d+\", \"\", text_example)\n",
    "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    dict['kalimat ' + str(no)]['lower_punct'] = text\n",
    "    no +=1\n",
    "\n",
    "title_lower = title.lower()\n",
    "count_title = len(title_lower.split())\n",
    "\n",
    "\n",
    "# print(count_title)\n",
    "\n",
    "# print(re_punct)\n",
    "\n",
    "# print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi\n",
    "from nltk.tokenize import word_tokenize\n",
    "bag_of_word = []\n",
    "\n",
    "no = 1\n",
    "for dic in dict:\n",
    "    token = word_tokenize(dict['kalimat ' + str(no)]['lower_punct'])\n",
    "    dict['kalimat ' + str(no)]['token'] = token\n",
    "    bag_of_word.append(token)\n",
    "    no += 1\n",
    "# print(dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get Indonesian stopword \n",
    "list_stopwords = set(stopwords.words('indonesian'))\n",
    "no = 1\n",
    "# print(dict['kalimat 1']['token'])\n",
    "for kalimat in dict:\n",
    "    for token in dict['kalimat ' + str(no)]['token']:\n",
    "        new_token = [token for token in dict['kalimat ' + str(no)]['token'] if not token in list_stopwords]\n",
    "        dict['kalimat ' + str(no)]['token'] = new_token\n",
    "    # print(dict[kalimat]['token'])\n",
    "    # print(new_token)\n",
    "\n",
    "#remove stopword pada list token\n",
    "# tokens_without_stopword = [word for word in  if not word in list_stopwords]\n",
    "\n",
    "\n",
    "# print(tokens_without_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "for x in range(len(dict)):\n",
    "    token = dict['kalimat ' + str(x+1)]['token']\n",
    "    new_token = []\n",
    "    for i in token: \n",
    "        hasil = stemmer.stem(i)\n",
    "#         # print(i + ': '+ hasil)\n",
    "        new_token.append(stemmer.stem(i))\n",
    "    dict['kalimat ' + str(x+1)]['token'] = new_token\n",
    "\n",
    "# print(dict)\n",
    "# for x in range(len(dict)):\n",
    "#     print(x)\n",
    "# print(len(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Word Proccess\n",
    "bag_of_word = []\n",
    "no = 1\n",
    "for token in dict:\n",
    "    bag_of_word += dict['kalimat ' + str(no)]['token']\n",
    "    no += 1\n",
    "# Remove duplicate of bag_of_word\n",
    "new_bag_of_word = []\n",
    "[new_bag_of_word.append(x) for x in bag_of_word if x not in new_bag_of_word]\n",
    "# [for x in bag_of_word if x not in new_bag_of_word]\n",
    "# print(new_bag_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF\n",
    "# print(new_bag_of_word[0])\n",
    "\n",
    "for kalimat in range(len(dict)):\n",
    "    token = dict['kalimat ' + str(kalimat + 1)]['token']\n",
    "    time_freq = {}\n",
    "    for word in new_bag_of_word:\n",
    "        count = 0\n",
    "        for x in token:\n",
    "            if(x == word):\n",
    "                count += 1\n",
    "        time_freq[word] = count / len(token)\n",
    "            # time_freq[x] = new_bag_of_word.count(x) / len(token)\n",
    "    dict['kalimat ' + str(kalimat + 1)]['time_freq_normalized'] = time_freq\n",
    "# print(token)\n",
    "# print(new_bag_of_word)\n",
    "\n",
    "# print(dict)\n",
    "# for x in range(len(dict)):\n",
    "#     time_freq = {}\n",
    "#     for token in dict['kalimat ' + str(x+1)]['token']:\n",
    "#         new_bag_of_word[x].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF\n",
    "# print(new_bag_of_word)\n",
    "df = {}\n",
    "for word in new_bag_of_word:\n",
    "    count = 0\n",
    "    for x in dict:\n",
    "        for y in dict[x]['token']:\n",
    "            if word == y:\n",
    "                count +=1\n",
    "        df[word] = count\n",
    "# print(df) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF\n",
    "import math\n",
    "\n",
    "idf = {}\n",
    "for word in new_bag_of_word:\n",
    "    idf[word] = math.log(len(dict)/df[word])\n",
    "\n",
    "# print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF.IDF\n",
    "all_tf_idf =[]\n",
    "# tf_idf =  []\n",
    "for kalimat in dict:\n",
    "    tf_idf_sentence_with_sentence = {}\n",
    "    tf_idf_sentence = []\n",
    "    tf = dict[kalimat]['time_freq_normalized']\n",
    "    # print(type(tf_idf_sentence))\n",
    "    for word in new_bag_of_word:\n",
    "        tf_idf = tf[word] * idf[word]\n",
    "        tf_idf_sentence.append(tf_idf)\n",
    "        tf_idf_sentence_with_sentence[word] = tf_idf \n",
    "\n",
    "    dict[kalimat]['TF-IDF'] = tf_idf_sentence\n",
    "    dict[kalimat]['TF-IDF with sentence'] = tf_idf_sentence_with_sentence\n",
    "\n",
    "    # print(tf_idf_sentence)\n",
    "    # all_tf_idf.append(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUM TF-IDF per kalimat\n",
    "for kalimat in dict:\n",
    "    tf_idf = dict[kalimat]['TF-IDF']\n",
    "    # print(len(tf_idf))\n",
    "    sum = 0\n",
    "    for n in range(len(tf_idf)):\n",
    "        sum += tf_idf[n]\n",
    "    dict[kalimat]['Sum TF-IDF'] = sum\n",
    "    # print('TF-IDF ' + kalimat + ' : ' + str(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity\n",
    "import math\n",
    "# # print(len(kalimat_1))\n",
    "# print(len(kalimat_2))\n",
    "# print(dict['kalimat 15'])\n",
    "\n",
    "    # kalimat_2 = dict['kalimat 2']['TF-IDF']\n",
    "for kalimat1 in dict:\n",
    "\n",
    "    kalimat_1 = dict[kalimat1]['TF-IDF']\n",
    "    # cosine_similarity = {} \n",
    "    dict[kalimat1]['cosine_similarity'] = {}\n",
    "\n",
    "    for kalimat2 in (dict): \n",
    "        kalimat_2 = dict[kalimat2]['TF-IDF']\n",
    "        \n",
    "        \n",
    "\n",
    "        ab = [] # atas\n",
    "        a = [] # ||A||\n",
    "        b = [] # ||B||\n",
    "\n",
    "        if kalimat_2 != kalimat_1:\n",
    "        # Calculate Cosine\n",
    "            # cosine_similarity= {}\n",
    "            for x in range(len(kalimat_1)):\n",
    "                ab.append(kalimat_1[x] * kalimat_2[x]) # A * B\n",
    "                a.append(pow(kalimat_1[x],2)) # ||A||\n",
    "                b.append(pow(kalimat_2[x],2)) # ||B||\n",
    "            \n",
    "            sum_ab = 0 # init sum A * B\n",
    "            for y in (ab):\n",
    "               sum_ab += y \n",
    "                \n",
    "            sum_a = 0 # init sum ||A||\n",
    "            sum_b = 0 # init sum ||B||\n",
    "\n",
    "            for z in (a):\n",
    "                sum_a += z\n",
    "\n",
    "            for z in (b):\n",
    "                sum_b += z\n",
    "        \n",
    "            a_b = math.sqrt(sum_a) * math.sqrt(sum_b) # ||A|| * ||B||\n",
    "            cosine_similarity = sum_ab / a_b\n",
    "            dict[kalimat1]['cosine_similarity'][kalimat2] = cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print('||A||')\n",
    "# print(a)\n",
    "\n",
    "# print('||B||')\n",
    "# print(b)\n",
    "\n",
    "\n",
    "\n",
    "# print('Jumlah ||A|| : ' + str(sum_a))\n",
    "# print('Jumlah ||B|| : ' + str(sum_b))\n",
    "# print('Jumlah A * B : ' + str(sum_ab))\n",
    "# print(' ||A|| * ||B|| : ' + str(a_b))\n",
    "\n",
    "# print('\\nCosine Similarity kalimat 1 dan 2 : ' + str(sum_ab / a_b))\n",
    "# print(sum_ab2)\n",
    "\n",
    "# print(round(3.4641016151377545870548926830117 * 3.1622776601683793319988935444327))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUMP cosine_similarity\n",
    "\n",
    "cosine_similarity = {}\n",
    "for x in dict:\n",
    "    cosine_similarity[x] = dict[x]['cosine_similarity']\n",
    "# print(cosine_similarity)\n",
    "\n",
    "# Export JSON\n",
    "import json\n",
    "\n",
    "with open('cosine_similarity.json', 'w') as json_file:\n",
    "    json.dump(cosine_similarity, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = dict['kalimat 1']['cosine_similarity']\n",
    "\n",
    "edge = [] \n",
    "for x in cosine_similarity:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "# Grapf\n",
    "grapf = []\n",
    "vertex = [] # Vertex\n",
    "edge = [] # Edge\n",
    "\n",
    "for i in range(len(dict)):\n",
    "    vertex.append(i + 1)\n",
    "\n",
    "for i in dict:\n",
    "    dict[i]['cosine_similarity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export JSON\n",
    "import json\n",
    "\n",
    "with open('dict.json', 'w') as json_file:\n",
    "    json.dump(dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orek - orek TF (boleh di skip)\n",
    "# print(new_bag_of_word)\n",
    "print(new_bag_of_word.count('negeri'))\n",
    "token = dict['kalimat 1']['token']\n",
    "print(len(token))\n",
    "list = {}\n",
    "for x in new_bag_of_word:\n",
    "    count = 0\n",
    "    for y in token:\n",
    "        if (y == x):\n",
    "            count += 1\n",
    "            # print(x + ':'+ y)\n",
    "    list[x] = count / len(token)\n",
    "\n",
    "# print(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tittle Similarity\n",
    "n = 1\n",
    "while n <= len(dict):\n",
    "    word_in_title = 0\n",
    "    title_similarity = 0.0\n",
    "    for j in dict['text' + str(n)]['lower_punct'].split():\n",
    "        for k in title_lower.split():\n",
    "            if j == k:\n",
    "                # print(k + ': ' + j)\n",
    "                word_in_title += 1\n",
    "    title_similarity = word_in_title / count_title\n",
    "    dict['text' + str(n)]['title_similarity'] = title_similarity\n",
    "    print(dict['text' + str(n)][\"title_similarity\"])\n",
    "    n += 1\n",
    "\n",
    "# for i in dict['text1']['lower_punct'].split():\n",
    "#     for j in title_lower.split():\n",
    "#         if i == j:\n",
    "#             print(j  +':' + j)\n",
    "#             word_in_title += 1\n",
    "#             title_similarity = word_in_title / count_title\n",
    "#             dict['text1']['title_similarity'] = title_similarity\n",
    "\n",
    "# print(title_similarity)\n",
    "# n = 1\n",
    "# for i in dict:\n",
    "#     print(dict['text']['title_similarity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Length of Text\n",
    "i = 1\n",
    "max_length = 0\n",
    "while i <= len(dict):\n",
    "    len_word = len(dict['text' + str(i)]['lower_punct'].split())\n",
    "    dict['text' + str(i)]['length'] = len_word\n",
    "    print(dict['text' + str(i)]['length'])\n",
    "    \n",
    "    if max_length < len_word:\n",
    "        max_length = len_word\n",
    "    i += 1\n",
    "print(\"Max length: \", max_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate L score for each sentence\n",
    "i = 1\n",
    "while i <= len(dict): \n",
    "    length_score = dict['text' +str(i)]['length'] / max_length\n",
    "    dict['text' + str(i)]['length_score'] = length_score\n",
    "    print(length_score)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Data to JSON model\n",
    "import json\n",
    "print(json.dumps(dict, indent= 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location feature\n",
    "# print(len(dict))\n",
    "total_sentence = len(dict)\n",
    "i = 1\n",
    "while i <= len(dict):\n",
    "    dict['text' + str(i)]['location_feature'] = 1 - (i / total_sentence)\n",
    "    i += 1\n",
    "\n",
    "print(dict)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict['text2']['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fbfea1dd5a17f80dff8df3ba641602c59e31ce1a55b82aea18e6894ff3c71a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
