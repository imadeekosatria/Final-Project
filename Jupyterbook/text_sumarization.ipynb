{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "\n",
    "f = open('PNS Kemendagri Halalbihalal Lebaran 2022 di Metaverse.txt', encoding= 'utf-8')\n",
    "title = 'PNS Kemendagri Halalbihalal Lebaran 2022 di Metaverse'\n",
    "text = f.read()\n",
    "f.close()\n",
    "\n",
    "# Jadikan satu baris teks\n",
    "new_text =  ' '.join(text.splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentasi\n",
    "\n",
    "# import re\n",
    "dict = {}\n",
    "n = 1\n",
    "for i in new_text.split('.'):\n",
    "    for k in i.split('\\n'):\n",
    "        if i == '':\n",
    "            break\n",
    "        # i = re.sub('\\s+', ' ', i)\n",
    "        tes = {'kalimat' : i}\n",
    "        dict['kalimat ' + str(n)] = tes\n",
    "        n += 1\n",
    "# print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filterisasi dan lower case\n",
    "import string\n",
    "import re\n",
    "no = 1\n",
    "for dic in dict:\n",
    "    text_example = dict['kalimat ' + str(no)]['kalimat'].lower()\n",
    "    # print(text_example)\n",
    "    # re_punct = \"\".join([char for char in text_example if char not in string.punctuation])\n",
    "    text = re.sub(r\"\\d+\", \"\", text_example)\n",
    "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    dict['kalimat ' + str(no)]['lower_punct'] = text\n",
    "    no +=1\n",
    "\n",
    "title_lower = title.lower()\n",
    "count_title = len(title_lower.split())\n",
    "\n",
    "# print(len(count_title))\n",
    "\n",
    "# print(re_punct)\n",
    "\n",
    "# print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi\n",
    "from nltk.tokenize import word_tokenize\n",
    "bag_of_word = []\n",
    "\n",
    "no = 1\n",
    "for dic in dict:\n",
    "    token = word_tokenize(dict['kalimat ' + str(no)]['lower_punct'])\n",
    "    dict['kalimat ' + str(no)]['token'] = token\n",
    "    bag_of_word.append(token)\n",
    "    no += 1\n",
    "# print(dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get Indonesian stopword \n",
    "list_stopwords = set(stopwords.words('indonesian'))\n",
    "no = 1\n",
    "# print(dict['kalimat 1']['token'])\n",
    "for kalimat in dict:\n",
    "    for token in dict['kalimat ' + str(no)]['token']:\n",
    "        new_token = [token for token in dict['kalimat ' + str(no)]['token'] if not token in list_stopwords]\n",
    "        dict['kalimat ' + str(no)]['token'] = new_token\n",
    "    # print(dict[kalimat]['token'])\n",
    "    # print(new_token)\n",
    "\n",
    "#remove stopword pada list token\n",
    "# tokens_without_stopword = [word for word in  if not word in list_stopwords]\n",
    "\n",
    "\n",
    "# print(tokens_without_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "for x in range(len(dict)):\n",
    "    token = dict['kalimat ' + str(x+1)]['token']\n",
    "    new_token = []\n",
    "    for i in token: \n",
    "        hasil = stemmer.stem(i)\n",
    "#         # print(i + ': '+ hasil)\n",
    "        new_token.append(stemmer.stem(i))\n",
    "    dict['kalimat ' + str(x+1)]['token'] = new_token\n",
    "\n",
    "# print(dict)\n",
    "# for x in range(len(dict)):\n",
    "#     print(x)\n",
    "# print(len(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Word Proccess\n",
    "bag_of_word = []\n",
    "no = 1\n",
    "for token in dict:\n",
    "    bag_of_word += dict['kalimat ' + str(no)]['token']\n",
    "    no += 1\n",
    "# Remove duplicate of bag_of_word\n",
    "new_bag_of_word = []\n",
    "[new_bag_of_word.append(x) for x in bag_of_word if x not in new_bag_of_word]\n",
    "# [for x in bag_of_word if x not in new_bag_of_word]\n",
    "# print(new_bag_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF\n",
    "# print(new_bag_of_word[0])\n",
    "\n",
    "for kalimat in range(len(dict)):\n",
    "    token = dict['kalimat ' + str(kalimat + 1)]['token']\n",
    "    time_freq = {}\n",
    "    for word in new_bag_of_word:\n",
    "        count = 0\n",
    "        for x in token:\n",
    "            if(x == word):\n",
    "                count += 1\n",
    "        time_freq[word] = count / len(token)\n",
    "            # time_freq[x] = new_bag_of_word.count(x) / len(token)\n",
    "    dict['kalimat ' + str(kalimat + 1)]['time_freq_normalized'] = time_freq\n",
    "# print(token)\n",
    "# print(new_bag_of_word)\n",
    "\n",
    "# print(dict)\n",
    "# for x in range(len(dict)):\n",
    "#     time_freq = {}\n",
    "#     for token in dict['kalimat ' + str(x+1)]['token']:\n",
    "#         new_bag_of_word[x].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF\n",
    "# print(new_bag_of_word)\n",
    "df = {}\n",
    "for word in new_bag_of_word:\n",
    "    count = 0\n",
    "    for x in dict:\n",
    "        for y in dict[x]['token']:\n",
    "            if word == y:\n",
    "                count +=1\n",
    "        df[word] = count\n",
    "# print(df) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF\n",
    "import math\n",
    "\n",
    "idf = {}\n",
    "for word in new_bag_of_word:\n",
    "    idf[word] = math.log(len(dict)/df[word])\n",
    "\n",
    "# print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF.IDF\n",
    "all_tf_idf =[]\n",
    "# tf_idf =  []\n",
    "for kalimat in dict:\n",
    "    tf_idf_sentence_with_sentence = {}\n",
    "    tf_idf_sentence = []\n",
    "    tf = dict[kalimat]['time_freq_normalized']\n",
    "    # print(type(tf_idf_sentence))\n",
    "    for word in new_bag_of_word:\n",
    "        tf_idf = tf[word] * idf[word]\n",
    "        tf_idf_sentence.append(tf_idf)\n",
    "        tf_idf_sentence_with_sentence[word] = tf_idf \n",
    "\n",
    "    dict[kalimat]['TF-IDF'] = tf_idf_sentence\n",
    "    dict[kalimat]['TF-IDF with sentence'] = tf_idf_sentence_with_sentence\n",
    "\n",
    "    # print(tf_idf_sentence)\n",
    "    # all_tf_idf.append(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUM TF-IDF per kalimat\n",
    "for kalimat in dict:\n",
    "    tf_idf = dict[kalimat]['TF-IDF']\n",
    "    # print(len(tf_idf))\n",
    "    sum = 0\n",
    "    for n in range(len(tf_idf)):\n",
    "        sum += tf_idf[n]\n",
    "    dict[kalimat]['Sum TF-IDF'] = sum\n",
    "    # print('TF-IDF ' + kalimat + ' : ' + str(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tittle Similarity\n",
    "\n",
    "\n",
    "for kalimat in dict:\n",
    "    word_in_title = 0\n",
    "    sentence = dict[kalimat]['kalimat'].split()\n",
    "    for word in sentence:\n",
    "        for word_title in title_lower.split():\n",
    "            if word == word_title:\n",
    "                word_in_title += 1\n",
    "    title_similarity = word_in_title / len(title_lower.split())\n",
    "    dict[kalimat]['title_similarity'] = title_similarity\n",
    "    # print(dict[kalimat]['title_similarity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Sentence length feature (L)\n",
    "\n",
    "# Max Length of Text\n",
    "\n",
    "max_length = 0\n",
    "for kalimat in dict:\n",
    "    length_sentence = dict[kalimat]['kalimat'].split()\n",
    "    if max_length <= len(length_sentence):\n",
    "        max_length = len(length_sentence)\n",
    "print(max_length)    \n",
    "\n",
    "# Length feature each sentence\n",
    "\n",
    "for kalimat in dict:\n",
    "    count = 0\n",
    "    sentence = len(dict[kalimat]['kalimat'].split())\n",
    "\n",
    "    \n",
    "    dict[kalimat]['length'] = sentence / max_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location feature\n",
    "\n",
    "for kalimat in dict:\n",
    "    loc_s = kalimat[8:] \n",
    "    loc = 1 - (int(loc_s) / len(dict))\n",
    "    dict[kalimat]['Loc'] = loc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISCORE\n",
    "\n",
    "for kalimat in dict:\n",
    "    t = dict[kalimat]['title_similarity']\n",
    "    l = dict[kalimat]['length']\n",
    "    loc = dict[kalimat]['Loc']\n",
    "    sum_tf_idf = dict[kalimat]['Sum TF-IDF']\n",
    "    iscore = t + l + loc + sum_tf_idf\n",
    "\n",
    "    dict[kalimat]['ISCORE'] = iscore \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity\n",
    "import math\n",
    "# # print(len(kalimat_1))\n",
    "# print(len(kalimat_2))\n",
    "# print(dict['kalimat 15'])\n",
    "\n",
    "    # kalimat_2 = dict['kalimat 2']['TF-IDF']\n",
    "for kalimat1 in dict:\n",
    "\n",
    "    kalimat_1 = dict[kalimat1]['TF-IDF']\n",
    "    # cosine_similarity = {} \n",
    "    dict[kalimat1]['cosine_similarity'] = {}\n",
    "    dict[kalimat1]['edge ' + kalimat1] = {}\n",
    "    for kalimat2 in (dict): \n",
    "        kalimat_2 = dict[kalimat2]['TF-IDF']\n",
    "        \n",
    "        \n",
    "\n",
    "        ab = [] # atas\n",
    "        a = [] # ||A||\n",
    "        b = [] # ||B||\n",
    "\n",
    "        if kalimat_2 != kalimat_1:\n",
    "        # Calculate Cosine\n",
    "            # cosine_similarity= {}\n",
    "            for x in range(len(kalimat_1)):\n",
    "                ab.append(kalimat_1[x] * kalimat_2[x]) # A * B\n",
    "                a.append(pow(kalimat_1[x],2)) # ||A||\n",
    "                b.append(pow(kalimat_2[x],2)) # ||B||\n",
    "            \n",
    "            sum_ab = 0 # init sum A * B\n",
    "            for y in (ab):\n",
    "               sum_ab += y \n",
    "                \n",
    "            sum_a = 0 # init sum ||A||\n",
    "            sum_b = 0 # init sum ||B||\n",
    "\n",
    "            for z in (a):\n",
    "                sum_a += z\n",
    "\n",
    "            for z in (b):\n",
    "                sum_b += z\n",
    "        \n",
    "            a_b = math.sqrt(sum_a) * math.sqrt(sum_b) # ||A|| * ||B||\n",
    "            cosine_similarity = sum_ab / a_b\n",
    "            dict[kalimat1]['cosine_similarity'][kalimat2] = cosine_similarity\n",
    "            dict[kalimat1]['edge ' + kalimat1][int(kalimat2[8:])] = cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUMP cosine_similarity\n",
    "\n",
    "cosine_similarity = {}\n",
    "for x in dict:\n",
    "    cosine_similarity[x] = dict[x]['cosine_similarity']\n",
    "# print(cosine_similarity)\n",
    "\n",
    "# Export JSON\n",
    "import json\n",
    "\n",
    "with open('cosine_similarity.json', 'w') as json_file:\n",
    "    json.dump(cosine_similarity, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (1, 3), (1, 4), (1, 5), (1, 8), (1, 9), (1, 11), (1, 12), (1, 13), (2, 4), (2, 5), (2, 7), (2, 8), (2, 9), (2, 13), (2, 14), (3, 4), (3, 5), (3, 6), (3, 7), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (4, 5), (4, 6), (4, 8), (4, 9), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 12), (5, 13), (5, 14), (6, 7), (6, 10), (6, 12), (6, 13), (6, 14), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 14), (8, 12), (8, 14), (9, 12), (10, 11), (10, 12), (10, 13), (10, 14), (11, 12), (11, 13), (11, 14), (12, 13), (13, 14)]\n"
     ]
    }
   ],
   "source": [
    "# Graf I\n",
    "# Edge\n",
    "\n",
    "for kalimat in dict:\n",
    "    edge_kalimat = []\n",
    "    kalimat1 = dict[kalimat]['cosine_similarity']\n",
    "    for key in kalimat1:\n",
    "        if (kalimat1[key] != 0):\n",
    "            tupple = (int(kalimat[8:]), int(key[8:]))\n",
    "            edge_kalimat.append(tupple)\n",
    "    dict[kalimat]['edge'] = edge_kalimat\n",
    "\n",
    "\n",
    "# print(edge)\n",
    "\n",
    "edge = []\n",
    "# Kumpulkan semua edge\n",
    "for kalimat in dict:\n",
    "    edge1 = dict[kalimat]['edge']\n",
    "    for e in edge1:\n",
    "        edge.append(e) \n",
    "                        \n",
    "# Hapus duplikat edge\n",
    "for e in edge:\n",
    "    a = e[0]\n",
    "    b = e[1]\n",
    "    for x in edge:\n",
    "        if e != x:\n",
    "            c = x[0]\n",
    "            d = x[1]\n",
    "            if a == d and b == c:\n",
    "                edge.remove(x)\n",
    "                    \n",
    "                        \n",
    "print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# Graf II\n",
    "\n",
    "vertex = [] # Vertex\n",
    "\n",
    "\n",
    "for i in range(len(dict)):\n",
    "    vertex.append(i + 1)\n",
    "print(vertex)\n",
    "print(len(vertex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def network(pos):\n",
    "    find_net = []\n",
    "    for ed in edge:\n",
    "        if ed[0] == pos:\n",
    "            find_net.append(ed)\n",
    "    return random.choice(find_net)\n",
    "\n",
    "def create_particles():\n",
    "    solution = []\n",
    "\n",
    "    for i in range(len(vertex)):\n",
    "        if not solution:\n",
    "            x = network(i + 1)\n",
    "            a = x[0]\n",
    "            solution.append(a)\n",
    "        elif solution[-1] != 14:\n",
    "            t = i - 1\n",
    "            x = network(solution[-1])\n",
    "        else:\n",
    "            break\n",
    "        b = x[1]\n",
    "        solution.append(b)\n",
    "        \n",
    "    return solution\n",
    "\n",
    "particle = {}\n",
    "for p in range(1,26):\n",
    "    par = create_particles()\n",
    "    if par not in particle.values():\n",
    "        particle[p] = par\n",
    "    else:\n",
    "        while par not in particle.values():\n",
    "            par = create_particles()\n",
    "        particle[p] = par\n",
    "\n",
    "\n",
    "\n",
    "# Export JSON\n",
    "import json\n",
    "\n",
    "with open('particle.json', 'w') as json_file:\n",
    "    json.dump(particle, json_file)\n",
    "\n",
    "\n",
    "# with open('res.json', 'w') as json_file:\n",
    "#     json.dump(res, json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [1,2]\n",
    "\n",
    "print(array[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# PSO\n",
    "\n",
    "# initialize parameters\n",
    "iterations = 100 \n",
    "population = 25  \n",
    "c1 = 1\n",
    "c2 = 2\n",
    "pbest = 0\n",
    "gbest = 0\n",
    "prev = 0\n",
    "v = 0\n",
    "def update(c1, c2, pbest, gbest, prev, v):\n",
    "    velocity = v + c1 * random.random() * (pbest - prev) + c2 * random.random() * (gbest - prev)\n",
    "\n",
    "    pos = prev + velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(particle[1])\n",
    "\n",
    "def fun_object(x, y):\n",
    "    iscore = dict['kalimat ' + str(x)]['ISCORE']\n",
    "    sim = cosine_similarity['kalimat ' + str(x)]['kalimat ' + str(y)]\n",
    "    return iscore * sim\n",
    "\n",
    "def update(c1, c2, pbest, gbest, prev, v):\n",
    "    velocity = v + c1 * random.random() * (pbest - prev) + c2 * random.random() * (gbest - prev)\n",
    "\n",
    "    pos = prev + velocity\n",
    "\n",
    "c1 = 1\n",
    "c2 = 2\n",
    "pbest = 0\n",
    "gbest = 0\n",
    "prev = 0\n",
    "v = 0\n",
    "\n",
    "\n",
    "fitness = []\n",
    "x = particle[1]\n",
    "a = x[0][0]\n",
    "b = x[0][1]\n",
    "# print(a,b)\n",
    "f = fun_object(a, b)\n",
    "# print(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export JSON\n",
    "import json\n",
    "\n",
    "with open('dict.json', 'w') as json_file:\n",
    "    json.dump(dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dict:\n",
    "    print(dict[d]['Loc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fbfea1dd5a17f80dff8df3ba641602c59e31ce1a55b82aea18e6894ff3c71a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
