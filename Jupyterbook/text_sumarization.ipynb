{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pegawai negeri sipil (PNS) Kementerian Dalam Negeri (Kemendagri) menggelar halalbihalal Idulfitri 1443 Hijriah di metaverse pada hari ini, Jumat (6/5). Direktur Jenderal Otonomi Daerah (Ditjen Otda) Kemendagri Akmal Malik mengatakan bahwa halalbihalal virtual itu lebih efisien. Dia mengklaim gelaran di metaverse ini pun dihadiri banyak pejabat dan PNS. \"Melalui pemanfaatan teknologi metaverse, pelaksanaan halalbihalal dapat lebih efisien baik dari segi waktu maupun tempat,\" kata Akmal dalam keterangan tertulis. Dalam foto yang dibagikan Pusat Penerangan Kemendagri, para PNS terlihat menggunakan kacamata virtual reality (VR) untuk menghadiri acara itu. Mereka hadir dalam ruangan maya dalam bentuk avatar. Ruangan maya itu terlihat seperti amphitheater dengan panggung di tengah. Terlihat pula baliho ucapan selamat Idulfitri lengkap dengan foto Akmal. Halalbihalal itu juga disertai tausiah oleh ustaz Das'ad Latif. Dia berceramah melalui avatar yang hadir dalam ruangan maya tersebut. \"Pertama kalinya saya bertausiah di hadapan jemaah melalui metaverse. Terlihat avatar diri saya dan para jemaah dalam ruangan metaverse yang disediakan,\" ucap Da'sad. Sebelumnya, Ditjen Otda Kemendagri sudah menerapkan sistem metaverse dalam layanan konsultasi pemda. Mereka mempersilakan pejabat daerah berkonsultasi dengan pejabat pusat melalui aplikasi Kovi Otda.\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "import os\n",
    "\n",
    "f = open('PNS Kemendagri Halalbihalal Lebaran 2022 di Metaverse.txt', encoding= 'utf-8')\n",
    "# title = 'PNS Kemendagri Halalbihalal Lebaran 2022 di Metaverse'\n",
    "text = f.read()\n",
    "# title = os.path.splitext(f.name)[0]\n",
    "title = 'PNS Kemendagri Halalbihalal Lebaran 2022 di Metaverse'\n",
    "f.close()\n",
    "\n",
    "# print(title)\n",
    "# Jadikan satu baris teks\n",
    "new_text =  ' '.join(text.splitlines())\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pegawai negeri sipil (PNS) Kementerian Dalam Negeri (Kemendagri) menggelar halalbihalal Idulfitri 1443 Hijriah di metaverse pada hari ini, Jumat (6/5).', 'Direktur Jenderal Otonomi Daerah (Ditjen Otda) Kemendagri Akmal Malik mengatakan bahwa halalbihalal virtual itu lebih efisien.', 'Dia mengklaim gelaran di metaverse ini pun dihadiri banyak pejabat dan PNS.', '\"Melalui pemanfaatan teknologi metaverse, pelaksanaan halalbihalal dapat lebih efisien baik dari segi waktu maupun tempat,\" kata Akmal dalam keterangan tertulis.', 'Dalam foto yang dibagikan Pusat Penerangan Kemendagri, para PNS terlihat menggunakan kacamata virtual reality (VR) untuk menghadiri acara itu.', 'Mereka hadir dalam ruangan maya dalam bentuk avatar.', 'Ruangan maya itu terlihat seperti amphitheater dengan panggung di tengah.', 'Terlihat pula baliho ucapan selamat Idulfitri lengkap dengan foto Akmal.', \"Halalbihalal itu juga disertai tausiah oleh ustaz Das'ad Latif.\", 'Dia berceramah melalui avatar yang hadir dalam ruangan maya tersebut.', '\"Pertama kalinya saya bertausiah di hadapan jemaah melalui metaverse.', 'Terlihat avatar diri saya dan para jemaah dalam ruangan metaverse yang disediakan,\" ucap Da\\'sad.', 'Sebelumnya, Ditjen Otda Kemendagri sudah menerapkan sistem metaverse dalam layanan konsultasi pemda.', 'Mereka mempersilakan pejabat daerah berkonsultasi dengan pejabat pusat melalui aplikasi Kovi Otda.']\n",
      "{'kalimat 1': {'kalimat': 'Pegawai negeri sipil (PNS) Kementerian Dalam Negeri (Kemendagri) menggelar halalbihalal Idulfitri 1443 Hijriah di metaverse pada hari ini, Jumat (6/5).'}, 'kalimat 2': {'kalimat': 'Direktur Jenderal Otonomi Daerah (Ditjen Otda) Kemendagri Akmal Malik mengatakan bahwa halalbihalal virtual itu lebih efisien.'}, 'kalimat 3': {'kalimat': 'Dia mengklaim gelaran di metaverse ini pun dihadiri banyak pejabat dan PNS.'}, 'kalimat 4': {'kalimat': '\"Melalui pemanfaatan teknologi metaverse, pelaksanaan halalbihalal dapat lebih efisien baik dari segi waktu maupun tempat,\" kata Akmal dalam keterangan tertulis.'}, 'kalimat 5': {'kalimat': 'Dalam foto yang dibagikan Pusat Penerangan Kemendagri, para PNS terlihat menggunakan kacamata virtual reality (VR) untuk menghadiri acara itu.'}, 'kalimat 6': {'kalimat': 'Mereka hadir dalam ruangan maya dalam bentuk avatar.'}, 'kalimat 7': {'kalimat': 'Ruangan maya itu terlihat seperti amphitheater dengan panggung di tengah.'}, 'kalimat 8': {'kalimat': 'Terlihat pula baliho ucapan selamat Idulfitri lengkap dengan foto Akmal.'}, 'kalimat 9': {'kalimat': \"Halalbihalal itu juga disertai tausiah oleh ustaz Das'ad Latif.\"}, 'kalimat 10': {'kalimat': 'Dia berceramah melalui avatar yang hadir dalam ruangan maya tersebut.'}, 'kalimat 11': {'kalimat': '\"Pertama kalinya saya bertausiah di hadapan jemaah melalui metaverse.'}, 'kalimat 12': {'kalimat': 'Terlihat avatar diri saya dan para jemaah dalam ruangan metaverse yang disediakan,\" ucap Da\\'sad.'}, 'kalimat 13': {'kalimat': 'Sebelumnya, Ditjen Otda Kemendagri sudah menerapkan sistem metaverse dalam layanan konsultasi pemda.'}, 'kalimat 14': {'kalimat': 'Mereka mempersilakan pejabat daerah berkonsultasi dengan pejabat pusat melalui aplikasi Kovi Otda.'}}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "segment_sentence = sent_tokenize(new_text)\n",
    "print(segment_sentence)\n",
    "dict = {}\n",
    "for sentence in range(len(segment_sentence)):\n",
    "    dict['kalimat '+str(sentence+1)] = {}\n",
    "    dict['kalimat '+str(sentence+1)]['kalimat'] = segment_sentence[sentence]\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Segmentasi\n",
    "\n",
    "# # import re\n",
    "# dict = {}\n",
    "# n = 1\n",
    "# for i in new_text.split('. '):\n",
    "#     for k in i.split('\\n'):\n",
    "#         if i == '':\n",
    "#             break\n",
    "#         # i = re.sub('\\s+', ' ', i)\n",
    "#         tes = {'kalimat' : i}\n",
    "#         dict['kalimat ' + str(n)] = tes\n",
    "#         n += 1\n",
    "# # print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kalimat 1': {'kalimat': 'Pegawai negeri sipil (PNS) Kementerian Dalam Negeri (Kemendagri) menggelar halalbihalal Idulfitri 1443 Hijriah di metaverse pada hari ini, Jumat (6/5).', 'lower_punct': 'pegawai negeri sipil pns kementerian dalam negeri kemendagri menggelar halalbihalal idulfitri hijriah di metaverse pada hari ini jumat '}, 'kalimat 2': {'kalimat': 'Direktur Jenderal Otonomi Daerah (Ditjen Otda) Kemendagri Akmal Malik mengatakan bahwa halalbihalal virtual itu lebih efisien.', 'lower_punct': 'direktur jenderal otonomi daerah ditjen otda kemendagri akmal malik mengatakan bahwa halalbihalal virtual itu lebih efisien'}, 'kalimat 3': {'kalimat': 'Dia mengklaim gelaran di metaverse ini pun dihadiri banyak pejabat dan PNS.', 'lower_punct': 'dia mengklaim gelaran di metaverse ini pun dihadiri banyak pejabat dan pns'}, 'kalimat 4': {'kalimat': '\"Melalui pemanfaatan teknologi metaverse, pelaksanaan halalbihalal dapat lebih efisien baik dari segi waktu maupun tempat,\" kata Akmal dalam keterangan tertulis.', 'lower_punct': 'melalui pemanfaatan teknologi metaverse pelaksanaan halalbihalal dapat lebih efisien baik dari segi waktu maupun tempat kata akmal dalam keterangan tertulis'}, 'kalimat 5': {'kalimat': 'Dalam foto yang dibagikan Pusat Penerangan Kemendagri, para PNS terlihat menggunakan kacamata virtual reality (VR) untuk menghadiri acara itu.', 'lower_punct': 'dalam foto yang dibagikan pusat penerangan kemendagri para pns terlihat menggunakan kacamata virtual reality vr untuk menghadiri acara itu'}, 'kalimat 6': {'kalimat': 'Mereka hadir dalam ruangan maya dalam bentuk avatar.', 'lower_punct': 'mereka hadir dalam ruangan maya dalam bentuk avatar'}, 'kalimat 7': {'kalimat': 'Ruangan maya itu terlihat seperti amphitheater dengan panggung di tengah.', 'lower_punct': 'ruangan maya itu terlihat seperti amphitheater dengan panggung di tengah'}, 'kalimat 8': {'kalimat': 'Terlihat pula baliho ucapan selamat Idulfitri lengkap dengan foto Akmal.', 'lower_punct': 'terlihat pula baliho ucapan selamat idulfitri lengkap dengan foto akmal'}, 'kalimat 9': {'kalimat': \"Halalbihalal itu juga disertai tausiah oleh ustaz Das'ad Latif.\", 'lower_punct': 'halalbihalal itu juga disertai tausiah oleh ustaz dasad latif'}, 'kalimat 10': {'kalimat': 'Dia berceramah melalui avatar yang hadir dalam ruangan maya tersebut.', 'lower_punct': 'dia berceramah melalui avatar yang hadir dalam ruangan maya tersebut'}, 'kalimat 11': {'kalimat': '\"Pertama kalinya saya bertausiah di hadapan jemaah melalui metaverse.', 'lower_punct': 'pertama kalinya saya bertausiah di hadapan jemaah melalui metaverse'}, 'kalimat 12': {'kalimat': 'Terlihat avatar diri saya dan para jemaah dalam ruangan metaverse yang disediakan,\" ucap Da\\'sad.', 'lower_punct': 'terlihat avatar diri saya dan para jemaah dalam ruangan metaverse yang disediakan ucap dasad'}, 'kalimat 13': {'kalimat': 'Sebelumnya, Ditjen Otda Kemendagri sudah menerapkan sistem metaverse dalam layanan konsultasi pemda.', 'lower_punct': 'sebelumnya ditjen otda kemendagri sudah menerapkan sistem metaverse dalam layanan konsultasi pemda'}, 'kalimat 14': {'kalimat': 'Mereka mempersilakan pejabat daerah berkonsultasi dengan pejabat pusat melalui aplikasi Kovi Otda.', 'lower_punct': 'mereka mempersilakan pejabat daerah berkonsultasi dengan pejabat pusat melalui aplikasi kovi otda'}}\n"
     ]
    }
   ],
   "source": [
    "# Filterisasi dan lower case\n",
    "import string\n",
    "import re\n",
    "no = 1\n",
    "for dic in dict:\n",
    "    text_example = dict['kalimat ' + str(no)]['kalimat'].lower()\n",
    "    # print(text_example)\n",
    "    # re_punct = \"\".join([char for char in text_example if char not in string.punctuation])\n",
    "    text = re.sub(r\"\\d+\", \"\", text_example)\n",
    "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    dict['kalimat ' + str(no)]['lower_punct'] = text\n",
    "    no +=1\n",
    "\n",
    "title_lower = title.lower()\n",
    "count_title = len(title_lower.split())\n",
    "\n",
    "# print(len(count_title))\n",
    "\n",
    "# print(re_punct)\n",
    "\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi\n",
    "from nltk.tokenize import word_tokenize\n",
    "bag_of_word = []\n",
    "\n",
    "no = 1\n",
    "for dic in dict:\n",
    "    token = word_tokenize(dict['kalimat ' + str(no)]['lower_punct'])\n",
    "    dict['kalimat ' + str(no)]['token'] = token\n",
    "    bag_of_word.append(token)\n",
    "    no += 1\n",
    "print(dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get Indonesian stopword \n",
    "list_stopwords = set(stopwords.words('indonesian'))\n",
    "no = 1\n",
    "# print(dict['kalimat 1']['token'])\n",
    "for kalimat in dict:\n",
    "    for token in dict['kalimat ' + str(no)]['token']:\n",
    "        new_token = [token for token in dict['kalimat ' + str(no)]['token'] if not token in list_stopwords]\n",
    "        dict['kalimat ' + str(no)]['token'] = new_token\n",
    "    # print(dict[kalimat]['token'])\n",
    "    # print(new_token)\n",
    "\n",
    "#remove stopword pada list token\n",
    "# tokens_without_stopword = [word for word in  if not word in list_stopwords]\n",
    "\n",
    "\n",
    "# print(tokens_without_stopword)\n",
    "print(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "for x in range(len(dict)):\n",
    "    token = dict['kalimat ' + str(x+1)]['token']\n",
    "    new_token = []\n",
    "    for i in token: \n",
    "        hasil = stemmer.stem(i)\n",
    "#         # print(i + ': '+ hasil)\n",
    "        new_token.append(stemmer.stem(i))\n",
    "    dict['kalimat ' + str(x+1)]['token'] = new_token\n",
    "\n",
    "print(dict)\n",
    "# for x in range(len(dict)):\n",
    "#     print(x)\n",
    "# print(len(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Word Proccess\n",
    "bag_of_word = []\n",
    "no = 1\n",
    "for token in dict:\n",
    "    bag_of_word += dict['kalimat ' + str(no)]['token']\n",
    "    no += 1\n",
    "# Remove duplicate of bag_of_word\n",
    "new_bag_of_word = []\n",
    "[new_bag_of_word.append(x) for x in bag_of_word if x not in new_bag_of_word]\n",
    "# [for x in bag_of_word if x not in new_bag_of_word]\n",
    "print(new_bag_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF\n",
    "# print(new_bag_of_word[0])\n",
    "\n",
    "for kalimat in dict:\n",
    "    # print(kalimat)\n",
    "    token = dict[kalimat]['token']\n",
    "    time_freq = {}\n",
    "    for word in new_bag_of_word:\n",
    "        count = 0\n",
    "        for x in token:\n",
    "            if(x == word):\n",
    "                count += 1\n",
    "        # try:\n",
    "        time_freq[word] = count / len(token)\n",
    "        # except ZeroDivisionError:\n",
    "        #     print(len(token))\n",
    "        #     print(token)\n",
    "            # time_freq[x] = new_bag_of_word.count(x) / len(token)\n",
    "    dict[kalimat]['time_freq_normalized'] = time_freq\n",
    "# print(token)\n",
    "# print(new_bag_of_word)\n",
    "for x in range(1,4):\n",
    "    print(dict['kalimat '+str(x)]['time_freq_normalized'])\n",
    "    print()\n",
    "# print(dict)\n",
    "# for x in range(len(dict)):\n",
    "#     time_freq = {}\n",
    "#     for token in dict['kalimat ' + str(x+1)]['token']:\n",
    "#         new_bag_of_word[x].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF\n",
    "# print(new_bag_of_word)\n",
    "df = {}\n",
    "for word in new_bag_of_word:\n",
    "    count = 0\n",
    "    for x in dict:\n",
    "        for y in dict[x]['token']:\n",
    "            if word == y:\n",
    "                count +=1\n",
    "        df[word] = count\n",
    "print(df) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF\n",
    "import math\n",
    "\n",
    "idf = {}\n",
    "for word in new_bag_of_word:\n",
    "    idf[word] = math.log(len(dict)/df[word])\n",
    "\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF.IDF\n",
    "all_tf_idf =[]\n",
    "# tf_idf =  []\n",
    "for kalimat in dict:\n",
    "    tf_idf_sentence_with_sentence = {}\n",
    "    tf_idf_sentence = []\n",
    "    tf = dict[kalimat]['time_freq_normalized']\n",
    "    # print(type(tf_idf_sentence))\n",
    "    for word in new_bag_of_word:\n",
    "        tf_idf = tf[word] * idf[word]\n",
    "        tf_idf_sentence.append(tf_idf)\n",
    "        tf_idf_sentence_with_sentence[word] = tf_idf \n",
    "\n",
    "    dict[kalimat]['TF-IDF'] = tf_idf_sentence\n",
    "    dict[kalimat]['TF-IDF with sentence'] = tf_idf_sentence_with_sentence\n",
    "\n",
    "    # print(tf_idf_sentence)\n",
    "    # all_tf_idf.append(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUM TF-IDF per kalimat\n",
    "for kalimat in dict:\n",
    "    tf_idf = dict[kalimat]['TF-IDF']\n",
    "    # print(len(tf_idf))\n",
    "    sum = 0\n",
    "    for n in range(len(tf_idf)):\n",
    "        sum += tf_idf[n]\n",
    "    dict[kalimat]['Sum TF-IDF'] = sum\n",
    "    # print('TF-IDF ' + kalimat + ' : ' + str(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = {}\n",
    "for word in new_bag_of_word:\n",
    "    count = 0\n",
    "    for x in dict:\n",
    "        if word in dict[x]['token']:\n",
    "            count +=1\n",
    "    sf[word] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-ISF\n",
    "import math\n",
    "\n",
    "for s in dict:\n",
    "    tf_isf_sentence = {}\n",
    "    tf = dict[s]['time_freq_normalized']\n",
    "    for word in new_bag_of_word:\n",
    "        tf_isf = tf[word] * math.log(len(dict)/sf[word])\n",
    "        tf_isf_sentence[word] = tf_isf \n",
    "\n",
    "    # dict[kalimat]['TF-ISF'] = tf_idf_sentence\n",
    "    dict[s]['TF-ISF with sentence'] = tf_isf_sentence\n",
    "    sum = 0\n",
    "    for t in dict[s]['TF-ISF with sentence']: \n",
    "        sum += dict[s]['TF-ISF with sentence'][t]\n",
    "    dict[s]['Sum TF-ISF'] = sum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tittle Similarity\n",
    "for kalimat in dict:\n",
    "    word_in_title = 0\n",
    "    sentence = dict[kalimat]['kalimat'].split()\n",
    "    for word in sentence:\n",
    "        for word_title in title_lower.split():\n",
    "            if word == word_title:\n",
    "                word_in_title += 1\n",
    "    title_similarity = word_in_title / len(title_lower.split())\n",
    "    dict[kalimat]['title_similarity'] = title_similarity\n",
    "    # print(dict[kalimat]['title_similarity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence length feature (L)\n",
    "\n",
    "# Max Length of Text\n",
    "\n",
    "max_length = 0\n",
    "for kalimat in dict:\n",
    "    length_sentence = dict[kalimat]['kalimat'].split()\n",
    "    if max_length <= len(length_sentence):\n",
    "        max_length = len(length_sentence)\n",
    "print(max_length)    \n",
    "\n",
    "# Length feature each sentence\n",
    "\n",
    "for kalimat in dict:\n",
    "    count = 0\n",
    "    sentence = len(dict[kalimat]['kalimat'].split())\n",
    "\n",
    "    \n",
    "    dict[kalimat]['length'] = sentence / max_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location feature\n",
    "\n",
    "for kalimat in dict:\n",
    "    loc_s = kalimat[8:] \n",
    "    loc = 1 - (int(loc_s) / len(dict))\n",
    "    dict[kalimat]['Loc'] = loc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISCORE\n",
    "iscore_list = {}\n",
    "for kalimat in dict:\n",
    "    t = dict[kalimat]['title_similarity']\n",
    "    l = dict[kalimat]['length']\n",
    "    loc = dict[kalimat]['Loc']\n",
    "    sum_tf_idf = dict[kalimat]['Sum TF-IDF']\n",
    "    sum_tf_isf = dict[kalimat]['Sum TF-ISF']\n",
    "    iscore = t + l + loc + sum_tf_idf + sum_tf_isf\n",
    "\n",
    "    dict[kalimat]['ISCORE'] = iscore\n",
    "    iscore_list[kalimat] = iscore \n",
    "\n",
    "import json\n",
    "\n",
    "with open('ISCORE.json', 'w') as json_file:\n",
    "    json.dump(iscore_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity\n",
    "import math\n",
    "# # print(len(kalimat_1))\n",
    "# print(len(kalimat_2))\n",
    "# print(dict['kalimat 15'])\n",
    "\n",
    "    # kalimat_2 = dict['kalimat 2']['TF-IDF']\n",
    "for kalimat1 in dict:\n",
    "\n",
    "    kalimat_1 = dict[kalimat1]['TF-IDF']\n",
    "    # cosine_similarity = {} \n",
    "    dict[kalimat1]['cosine_similarity'] = {}\n",
    "    dict[kalimat1]['edge ' + kalimat1] = {}\n",
    "    for kalimat2 in (dict): \n",
    "        kalimat_2 = dict[kalimat2]['TF-IDF']\n",
    "        \n",
    "        \n",
    "\n",
    "        ab = [] # atas\n",
    "        a = [] # ||A||\n",
    "        b = [] # ||B||\n",
    "\n",
    "        if kalimat_2 != kalimat_1:\n",
    "        # Calculate Cosine\n",
    "            # cosine_similarity= {}\n",
    "            for x in range(len(kalimat_1)):\n",
    "                ab.append(kalimat_1[x] * kalimat_2[x]) # A * B\n",
    "                a.append(pow(kalimat_1[x],2)) # ||A||\n",
    "                b.append(pow(kalimat_2[x],2)) # ||B||\n",
    "            \n",
    "            sum_ab = 0 # init sum A * B\n",
    "            for y in (ab):\n",
    "               sum_ab += y \n",
    "                \n",
    "            sum_a = 0 # init sum ||A||\n",
    "            sum_b = 0 # init sum ||B||\n",
    "\n",
    "            for z in (a):\n",
    "                sum_a += z\n",
    "\n",
    "            for z in (b):\n",
    "                sum_b += z\n",
    "        \n",
    "            a_b = math.sqrt(sum_a) * math.sqrt(sum_b) # ||A|| * ||B||\n",
    "            cosine_similarity = sum_ab / a_b\n",
    "            dict[kalimat1]['cosine_similarity'][kalimat2] = cosine_similarity\n",
    "            dict[kalimat1]['edge ' + kalimat1][int(kalimat2[8:])] = cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUMP cosine_similarity\n",
    "\n",
    "cosine_similarity = {}\n",
    "for x in dict:\n",
    "    cosine_similarity[x] = dict[x]['cosine_similarity']\n",
    "# print(cosine_similarity)\n",
    "\n",
    "# Export JSON\n",
    "import json\n",
    "\n",
    "with open('cosine_similarity.json', 'w') as json_file:\n",
    "    json.dump(cosine_similarity, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graf I\n",
    "# Edge\n",
    "\n",
    "for kalimat in dict:\n",
    "    edge_kalimat = []\n",
    "    kalimat1 = dict[kalimat]['cosine_similarity']\n",
    "    for key in kalimat1:\n",
    "        if (kalimat1[key] != 0):\n",
    "            tupple = (int(kalimat[8:]), int(key[8:]))\n",
    "            edge_kalimat.append(tupple)\n",
    "    dict[kalimat]['edge'] = edge_kalimat\n",
    "\n",
    "\n",
    "# print(edge)\n",
    "\n",
    "edge = []\n",
    "# Kumpulkan semua edge\n",
    "for kalimat in dict:\n",
    "    edge1 = dict[kalimat]['edge']\n",
    "    for e in edge1:\n",
    "        edge.append(e) \n",
    "                        \n",
    "# Hapus duplikat edge\n",
    "for e in edge:\n",
    "    a = e[0]\n",
    "    b = e[1]\n",
    "    for x in edge:\n",
    "        if e != x:\n",
    "            c = x[0]\n",
    "            d = x[1]\n",
    "            if a == d and b == c:\n",
    "                edge.remove(x)\n",
    "                    \n",
    "                        \n",
    "print(edge)\n",
    "print()\n",
    "print(len(edge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('edge.json', 'w') as json_file:\n",
    "    json.dump(edge, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graf II\n",
    "\n",
    "vertex = [] # Vertex\n",
    "\n",
    "\n",
    "for i in range(len(dict)):\n",
    "    vertex.append(i + 1)\n",
    "print(vertex)\n",
    "print(len(vertex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "particle = {}\n",
    "for i in range(1,6):\n",
    "    jalur = []\n",
    "    while len(jalur) != 3: #math.ceil(6): # <--- len(vertex)*2/10 atau int 4\n",
    "        r = random.choice(vertex)\n",
    "        if r not in jalur:\n",
    "            jalur.append(r)\n",
    "        jalur.sort()\n",
    "        \n",
    "    particle[i] = jalur\n",
    "\n",
    "# Export JSON\n",
    "import json\n",
    "\n",
    "with open('particle.json', 'w') as json_file:\n",
    "    json.dump(particle, json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# new_vertex = vertex[:14]\n",
    "\n",
    "# def network(pos):\n",
    "#     find_net = []\n",
    "#     for ed in edge:\n",
    "#         if ed[0] == pos:\n",
    "#             find_net.append(ed)\n",
    "#     return random.choice(find_net)\n",
    "\n",
    "# def create_particles():\n",
    "#     solution = []\n",
    "\n",
    "#     for i in range(len(vertex)):\n",
    "#         if not solution:\n",
    "#             x = network(random.choice(new_vertex))\n",
    "#             a = x[0]\n",
    "#             solution.append(a)\n",
    "#         elif solution[-1] != 14:\n",
    "#             t = i - 1\n",
    "#             x = network(solution[-1])\n",
    "#         else:\n",
    "#             break\n",
    "#         b = x[1]\n",
    "#         solution.append(b)\n",
    "        \n",
    "#     return solution\n",
    "\n",
    "# particle = {}\n",
    "# for p in range(1,6):\n",
    "#     par = create_particles()\n",
    "#     while len(par) < 5:\n",
    "#         par = create_particles()\n",
    "#     particle[p] = par\n",
    "#     # if par not in particle.values():\n",
    "#     #     particle[p] = par\n",
    "#     # else:\n",
    "#     #     while par not in particle.values():\n",
    "#     #         par = create_particles()\n",
    "#     #     particle[p] = par\n",
    "\n",
    "\n",
    "\n",
    "# # Export JSON\n",
    "# import json\n",
    "\n",
    "# with open('particle.json', 'w') as json_file:\n",
    "#     json.dump(particle, json_file)\n",
    "\n",
    "\n",
    "# # with open('res.json', 'w') as json_file:\n",
    "# #     json.dump(res, json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tes = []\n",
    "# while len(tes) < 4:\n",
    "#     i = random.choice(new_vertex)\n",
    "#     tes.append(i)\n",
    "# print(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # PSO\n",
    "\n",
    "# # initialize parameters\n",
    "# iterations = 100 \n",
    "# population = 25  \n",
    "# c1 = 1\n",
    "# c2 = 2\n",
    "# pbest = 0\n",
    "# gbest = 0\n",
    "# prev = 0\n",
    "# v = 0\n",
    "# # def update(c1, c2, pbest, gbest, prev, v):\n",
    "#     # velocity = v + c1 * random.random() * (pbest - prev) + c2 * random.random() * (gbest - prev)\n",
    "\n",
    "#     # pos = prev + velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(particle[1])\n",
    "\n",
    "# def fun_object(x, y):\n",
    "#     iscore = dict['kalimat ' + str(x)]['ISCORE']\n",
    "#     sim = cosine_similarity['kalimat ' + str(x)]['kalimat ' + str(y)]\n",
    "#     return iscore * sim\n",
    "\n",
    "# def update(c1, c2, pbest, gbest, prev, v):\n",
    "#     velocity = v + c1 * random.random() * (pbest - prev) + c2 * random.random() * (gbest - prev)\n",
    "\n",
    "#     pos = prev + velocity\n",
    "\n",
    "# c1 = 1\n",
    "# c2 = 2\n",
    "# pbest = 0\n",
    "# gbest = 0\n",
    "# prev = 0\n",
    "# v = 0\n",
    "\n",
    "\n",
    "# fitness = []\n",
    "# x = particle[1]\n",
    "# a = x[0][0]\n",
    "# b = x[0][1]\n",
    "# # print(a,b)\n",
    "# f = fun_object(a, b)\n",
    "# # print(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export JSON\n",
    "import json\n",
    "\n",
    "with open('dict.json', 'w') as json_file:\n",
    "    json.dump(dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in dict:\n",
    "#     print(dict[d]['Loc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3e10ef16274dd72e574b8fa73b58450b957d8421a2901baded3cca26fcf5dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
