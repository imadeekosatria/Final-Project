{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kasus Covid-19 di Indonesia kembali mengalami peningkatan dengan jumlah kasus harian beberapa hari terakhir tembus sebanyak 500 kasus. Menteri Kesehatan Budi Gunadi Sadikin mengatakan, kenaikan kasus ini imbas dari adanya subvarian Omicron BA.4 dan BA.5. \"Jadi kita confirm bahwa kenaikan ini memang dipicu oleh adanya varian baru dan ini juga yang terjadi sama di negara-negara di luar Indonesia yang mgkin hari raya keagamaannya berbeda-beda dengan kita. Jadi tiap kali ada varian baru itu (kasus) naik,\" kata Menteri Kesehatan (Menkes) Budi Gunadi Sadikin, dikutip dari Kompas.com, 13 Juni 2022. Lantas, apa perbedaan varian BA.4 dan BA.5 dengan varian sebelumnya? Terkait hal tersebut Kompas.com menghubungi Epidemiolog dari Griffith University Australia Dicky Budiman. Dia menjelaskan bahwa karakter subvarian BA.4 dan BA.5 berbeda dengan subvarian sebelumnya, terutama dengan BA.1. Khususnya BA.5 ini dia punya karakter yang merupakan kombinasi yangmana memiliki kecepatan lebih dari Omicron sebelumnya. Sehingga, mudah menginfeksi baik yang sudah vaksin ataupun belum, ujar Dicky, saat dihubungi Kompas.com, Selasa (14/6/2022). Dicky mengatakan bahwa BA.5 mengadopsi juga mutasi dari Delta L452 yang membuatnya mudah terikat di reseptor ACE 2 sehingga mudah masuk sel tubuh manusia. Riset terakhir di Jepang dan beberapa negara Eropa, BA.4 dan BA.5 kemampuan bereplikasinya di sel paru meningkat, ujarnya. Berdasarkan studi di laboratorium tersebut, BA.4 dan BA.5 lebih fusogenik dan patogenik jika dibandingkan dengan BA.2. Artinya potensi keparahan lebih infeksius. Yang jelas potensi keparahan ada, ungkap Dicky. Selain itu, dari studi juga ditemukan bahwa angka reproduksi efektivitas BA.4 dan BA.5 1,2 lebih tinggi jika dibandingkan dengan BA.2 atau subvarian lain. Artinya lebih cepat penularannya atau transmisinya karena kalau sudah angka reproduksi lebih dari 1 artinya ada pertumbuhan eksponensial yang akan terjadi, terangnya. Baca juga: 10 Gejala Omicron BA.4 dan BA.5 yang Terdeteksi di Bali dan Jakarta Dicky menyampaikan, berdasarkan studi, orang yang pernah terinfeksi BA.2 maupun BA.1 tak memiliki proteksi terhadap BA.4 dan BA.5. Meski demikian, ia menilai Indonesia selama dua tahun ini sudah terbentuk imunitas, sehingga penyebaran terjadi tetapi akan banyak orang yang tak bergejala. Apalagi Indonesia sudah pernah mengalami gelombang Delta. Kita diuntungkan pernah mengalami gelombang Delta yang besar, kemudian vaksinasi bergerak cepat, selain itu juga populasi muda kita besar, ujar Dicky. Ia menilai, saat ini negara yang memiliki risiko tinggi terhadap varian BA.4 dan BA.5 adalah negara yang walaupun memiliki cakupan vaksinasi lengkap dan booster 80 persen namun memiliki lansia dominan seperti Portugal. Selain itu, risiko juga terjadi pada negara yang belum mengalami dampak besar dari varian delta. Meski demikian, Dicky menyampaikan, perilaku adaptif terhadap pandemi, yakni memakai masker dan menjaga jarak tetap perlu untuk dilakukan masyarakat guna menghadapi subvarian baru ini.\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "import os\n",
    "\n",
    "f = open('Ini Beda Omicron Subvarian BA.4 dan BA.5 dengan Subvarian Sebelumnya.txt', encoding= 'utf-8')\n",
    "# title = 'PNS Kemendagri Halalbihalal Lebaran 2022 di Metaverse'\n",
    "text = f.read()\n",
    "title = os.path.splitext(f.name)[0]\n",
    "f.close()\n",
    "\n",
    "# print(title)\n",
    "# Jadikan satu baris teks\n",
    "new_text =  ' '.join(text.splitlines())\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "segment_sentence = sent_tokenize(new_text)\n",
    "# print(len(segment_sentence))\n",
    "dict = {}\n",
    "for sentence in range(len(segment_sentence)):\n",
    "    dict['kalimat '+str(sentence+1)] = {}\n",
    "    dict['kalimat '+str(sentence+1)]['kalimat'] = segment_sentence[sentence]\n",
    "# print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Segmentasi\n",
    "\n",
    "# # import re\n",
    "# dict = {}\n",
    "# n = 1\n",
    "# for i in new_text.split('. '):\n",
    "#     for k in i.split('\\n'):\n",
    "#         if i == '':\n",
    "#             break\n",
    "#         # i = re.sub('\\s+', ' ', i)\n",
    "#         tes = {'kalimat' : i}\n",
    "#         dict['kalimat ' + str(n)] = tes\n",
    "#         n += 1\n",
    "# # print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kalimat 1': {'kalimat': 'Kasus Covid-19 di Indonesia kembali mengalami peningkatan dengan jumlah kasus harian beberapa hari terakhir tembus sebanyak 500 kasus.', 'lower_punct': 'kasus covid di indonesia kembali mengalami peningkatan dengan jumlah kasus harian beberapa hari terakhir tembus sebanyak kasus'}, 'kalimat 2': {'kalimat': 'Menteri Kesehatan Budi Gunadi Sadikin mengatakan, kenaikan kasus ini imbas dari adanya subvarian Omicron BA.4 dan BA.5.', 'lower_punct': 'menteri kesehatan budi gunadi sadikin mengatakan kenaikan kasus ini imbas dari adanya subvarian omicron ba dan ba'}, 'kalimat 3': {'kalimat': '\"Jadi kita confirm bahwa kenaikan ini memang dipicu oleh adanya varian baru dan ini juga yang terjadi sama di negara-negara di luar Indonesia yang mgkin hari raya keagamaannya berbeda-beda dengan kita.', 'lower_punct': 'jadi kita confirm bahwa kenaikan ini memang dipicu oleh adanya varian baru dan ini juga yang terjadi sama di negaranegara di luar indonesia yang mgkin hari raya keagamaannya berbedabeda dengan kita'}, 'kalimat 4': {'kalimat': 'Jadi tiap kali ada varian baru itu (kasus) naik,\" kata Menteri Kesehatan (Menkes) Budi Gunadi Sadikin, dikutip dari Kompas.com, 13 Juni 2022.', 'lower_punct': 'jadi tiap kali ada varian baru itu kasus naik kata menteri kesehatan menkes budi gunadi sadikin dikutip dari kompascom juni '}, 'kalimat 5': {'kalimat': 'Lantas, apa perbedaan varian BA.4 dan BA.5 dengan varian sebelumnya?', 'lower_punct': 'lantas apa perbedaan varian ba dan ba dengan varian sebelumnya'}, 'kalimat 6': {'kalimat': 'Terkait hal tersebut Kompas.com menghubungi Epidemiolog dari Griffith University Australia Dicky Budiman.', 'lower_punct': 'terkait hal tersebut kompascom menghubungi epidemiolog dari griffith university australia dicky budiman'}, 'kalimat 7': {'kalimat': 'Dia menjelaskan bahwa karakter subvarian BA.4 dan BA.5 berbeda dengan subvarian sebelumnya, terutama dengan BA.1.', 'lower_punct': 'dia menjelaskan bahwa karakter subvarian ba dan ba berbeda dengan subvarian sebelumnya terutama dengan ba'}, 'kalimat 8': {'kalimat': 'Khususnya BA.5 ini dia punya karakter yang merupakan kombinasi yangmana memiliki kecepatan lebih dari Omicron sebelumnya.', 'lower_punct': 'khususnya ba ini dia punya karakter yang merupakan kombinasi yangmana memiliki kecepatan lebih dari omicron sebelumnya'}, 'kalimat 9': {'kalimat': 'Sehingga, mudah menginfeksi baik yang sudah vaksin ataupun belum, ujar Dicky, saat dihubungi Kompas.com, Selasa (14/6/2022).', 'lower_punct': 'sehingga mudah menginfeksi baik yang sudah vaksin ataupun belum ujar dicky saat dihubungi kompascom selasa '}, 'kalimat 10': {'kalimat': 'Dicky mengatakan bahwa BA.5 mengadopsi juga mutasi dari Delta L452 yang membuatnya mudah terikat di reseptor ACE 2 sehingga mudah masuk sel tubuh manusia.', 'lower_punct': 'dicky mengatakan bahwa ba mengadopsi juga mutasi dari delta l yang membuatnya mudah terikat di reseptor ace sehingga mudah masuk sel tubuh manusia'}, 'kalimat 11': {'kalimat': 'Riset terakhir di Jepang dan beberapa negara Eropa, BA.4 dan BA.5 kemampuan bereplikasinya di sel paru meningkat, ujarnya.', 'lower_punct': 'riset terakhir di jepang dan beberapa negara eropa ba dan ba kemampuan bereplikasinya di sel paru meningkat ujarnya'}, 'kalimat 12': {'kalimat': 'Berdasarkan studi di laboratorium tersebut, BA.4 dan BA.5 lebih fusogenik dan patogenik jika dibandingkan dengan BA.2.', 'lower_punct': 'berdasarkan studi di laboratorium tersebut ba dan ba lebih fusogenik dan patogenik jika dibandingkan dengan ba'}, 'kalimat 13': {'kalimat': 'Artinya potensi keparahan lebih infeksius.', 'lower_punct': 'artinya potensi keparahan lebih infeksius'}, 'kalimat 14': {'kalimat': 'Yang jelas potensi keparahan ada, ungkap Dicky.', 'lower_punct': 'yang jelas potensi keparahan ada ungkap dicky'}, 'kalimat 15': {'kalimat': 'Selain itu, dari studi juga ditemukan bahwa angka reproduksi efektivitas BA.4 dan BA.5 1,2 lebih tinggi jika dibandingkan dengan BA.2 atau subvarian lain.', 'lower_punct': 'selain itu dari studi juga ditemukan bahwa angka reproduksi efektivitas ba dan ba lebih tinggi jika dibandingkan dengan ba atau subvarian lain'}, 'kalimat 16': {'kalimat': 'Artinya lebih cepat penularannya atau transmisinya karena kalau sudah angka reproduksi lebih dari 1 artinya ada pertumbuhan eksponensial yang akan terjadi, terangnya.', 'lower_punct': 'artinya lebih cepat penularannya atau transmisinya karena kalau sudah angka reproduksi lebih dari artinya ada pertumbuhan eksponensial yang akan terjadi terangnya'}, 'kalimat 17': {'kalimat': 'Baca juga: 10 Gejala Omicron BA.4 dan BA.5 yang Terdeteksi di Bali dan Jakarta Dicky menyampaikan, berdasarkan studi, orang yang pernah terinfeksi BA.2 maupun BA.1 tak memiliki proteksi terhadap BA.4 dan BA.5.', 'lower_punct': 'baca juga gejala omicron ba dan ba yang terdeteksi di bali dan jakarta dicky menyampaikan berdasarkan studi orang yang pernah terinfeksi ba maupun ba tak memiliki proteksi terhadap ba dan ba'}, 'kalimat 18': {'kalimat': 'Meski demikian, ia menilai Indonesia selama dua tahun ini sudah terbentuk imunitas, sehingga penyebaran terjadi tetapi akan banyak orang yang tak bergejala.', 'lower_punct': 'meski demikian ia menilai indonesia selama dua tahun ini sudah terbentuk imunitas sehingga penyebaran terjadi tetapi akan banyak orang yang tak bergejala'}, 'kalimat 19': {'kalimat': 'Apalagi Indonesia sudah pernah mengalami gelombang Delta.', 'lower_punct': 'apalagi indonesia sudah pernah mengalami gelombang delta'}, 'kalimat 20': {'kalimat': 'Kita diuntungkan pernah mengalami gelombang Delta yang besar, kemudian vaksinasi bergerak cepat, selain itu juga populasi muda kita besar, ujar Dicky.', 'lower_punct': 'kita diuntungkan pernah mengalami gelombang delta yang besar kemudian vaksinasi bergerak cepat selain itu juga populasi muda kita besar ujar dicky'}, 'kalimat 21': {'kalimat': 'Ia menilai, saat ini negara yang memiliki risiko tinggi terhadap varian BA.4 dan BA.5 adalah negara yang walaupun memiliki cakupan vaksinasi lengkap dan booster 80 persen namun memiliki lansia dominan seperti Portugal.', 'lower_punct': 'ia menilai saat ini negara yang memiliki risiko tinggi terhadap varian ba dan ba adalah negara yang walaupun memiliki cakupan vaksinasi lengkap dan booster persen namun memiliki lansia dominan seperti portugal'}, 'kalimat 22': {'kalimat': 'Selain itu, risiko juga terjadi pada negara yang belum mengalami dampak besar dari varian delta.', 'lower_punct': 'selain itu risiko juga terjadi pada negara yang belum mengalami dampak besar dari varian delta'}, 'kalimat 23': {'kalimat': 'Meski demikian, Dicky menyampaikan, perilaku adaptif terhadap pandemi, yakni memakai masker dan menjaga jarak tetap perlu untuk dilakukan masyarakat guna menghadapi subvarian baru ini.', 'lower_punct': 'meski demikian dicky menyampaikan perilaku adaptif terhadap pandemi yakni memakai masker dan menjaga jarak tetap perlu untuk dilakukan masyarakat guna menghadapi subvarian baru ini'}}\n"
     ]
    }
   ],
   "source": [
    "# Filterisasi dan lower case\n",
    "import string\n",
    "import re\n",
    "no = 1\n",
    "for dic in dict:\n",
    "    text_example = dict['kalimat ' + str(no)]['kalimat'].lower()\n",
    "    # print(text_example)\n",
    "    # re_punct = \"\".join([char for char in text_example if char not in string.punctuation])\n",
    "    text = re.sub(r\"\\d+\", \"\", text_example)\n",
    "    text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    dict['kalimat ' + str(no)]['lower_punct'] = text\n",
    "    no +=1\n",
    "\n",
    "title_lower = title.lower()\n",
    "count_title = len(title_lower.split())\n",
    "\n",
    "# print(len(count_title))\n",
    "\n",
    "# print(re_punct)\n",
    "\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi\n",
    "from nltk.tokenize import word_tokenize\n",
    "bag_of_word = []\n",
    "\n",
    "no = 1\n",
    "for dic in dict:\n",
    "    token = word_tokenize(dict['kalimat ' + str(no)]['lower_punct'])\n",
    "    dict['kalimat ' + str(no)]['token'] = token\n",
    "    bag_of_word.append(token)\n",
    "    no += 1\n",
    "# print(dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get Indonesian stopword \n",
    "list_stopwords = set(stopwords.words('indonesian'))\n",
    "no = 1\n",
    "# print(dict['kalimat 1']['token'])\n",
    "for kalimat in dict:\n",
    "    for token in dict['kalimat ' + str(no)]['token']:\n",
    "        new_token = [token for token in dict['kalimat ' + str(no)]['token'] if not token in list_stopwords]\n",
    "        dict['kalimat ' + str(no)]['token'] = new_token\n",
    "    # print(dict[kalimat]['token'])\n",
    "    # print(new_token)\n",
    "\n",
    "#remove stopword pada list token\n",
    "# tokens_without_stopword = [word for word in  if not word in list_stopwords]\n",
    "\n",
    "\n",
    "# print(tokens_without_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "for x in range(len(dict)):\n",
    "    token = dict['kalimat ' + str(x+1)]['token']\n",
    "    new_token = []\n",
    "    for i in token: \n",
    "        hasil = stemmer.stem(i)\n",
    "#         # print(i + ': '+ hasil)\n",
    "        new_token.append(stemmer.stem(i))\n",
    "    dict['kalimat ' + str(x+1)]['token'] = new_token\n",
    "\n",
    "# print(dict)\n",
    "# for x in range(len(dict)):\n",
    "#     print(x)\n",
    "# print(len(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Word Proccess\n",
    "bag_of_word = []\n",
    "no = 1\n",
    "for token in dict:\n",
    "    bag_of_word += dict['kalimat ' + str(no)]['token']\n",
    "    no += 1\n",
    "# Remove duplicate of bag_of_word\n",
    "new_bag_of_word = []\n",
    "[new_bag_of_word.append(x) for x in bag_of_word if x not in new_bag_of_word]\n",
    "# [for x in bag_of_word if x not in new_bag_of_word]\n",
    "# print(new_bag_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kalimat 1\n",
      "kalimat 2\n",
      "kalimat 3\n",
      "kalimat 4\n",
      "kalimat 5\n",
      "kalimat 6\n",
      "kalimat 7\n",
      "kalimat 8\n",
      "kalimat 9\n",
      "kalimat 10\n",
      "kalimat 11\n",
      "kalimat 12\n",
      "kalimat 13\n",
      "kalimat 14\n",
      "kalimat 15\n",
      "kalimat 16\n",
      "kalimat 17\n",
      "kalimat 18\n",
      "kalimat 19\n",
      "kalimat 20\n",
      "kalimat 21\n",
      "kalimat 22\n",
      "kalimat 23\n"
     ]
    }
   ],
   "source": [
    "#TF\n",
    "# print(new_bag_of_word[0])\n",
    "\n",
    "for kalimat in dict:\n",
    "    print(kalimat)\n",
    "    token = dict[kalimat]['token']\n",
    "    time_freq = {}\n",
    "    for word in new_bag_of_word:\n",
    "        count = 0\n",
    "        for x in token:\n",
    "            if(x == word):\n",
    "                count += 1\n",
    "        # try:\n",
    "        time_freq[word] = count / len(token)\n",
    "        # except ZeroDivisionError:\n",
    "        #     print(len(token))\n",
    "        #     print(token)\n",
    "            # time_freq[x] = new_bag_of_word.count(x) / len(token)\n",
    "    dict[kalimat]['time_freq_normalized'] = time_freq\n",
    "# print(token)\n",
    "# print(new_bag_of_word)\n",
    "\n",
    "# print(dict)\n",
    "# for x in range(len(dict)):\n",
    "#     time_freq = {}\n",
    "#     for token in dict['kalimat ' + str(x+1)]['token']:\n",
    "#         new_bag_of_word[x].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DF\n",
    "# print(new_bag_of_word)\n",
    "df = {}\n",
    "for word in new_bag_of_word:\n",
    "    count = 0\n",
    "    for x in dict:\n",
    "        for y in dict[x]['token']:\n",
    "            if word == y:\n",
    "                count +=1\n",
    "        df[word] = count\n",
    "# print(df) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF\n",
    "import math\n",
    "\n",
    "idf = {}\n",
    "for word in new_bag_of_word:\n",
    "    idf[word] = math.log(len(dict)/df[word])\n",
    "\n",
    "# print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF.IDF\n",
    "all_tf_idf =[]\n",
    "# tf_idf =  []\n",
    "for kalimat in dict:\n",
    "    tf_idf_sentence_with_sentence = {}\n",
    "    tf_idf_sentence = []\n",
    "    tf = dict[kalimat]['time_freq_normalized']\n",
    "    # print(type(tf_idf_sentence))\n",
    "    for word in new_bag_of_word:\n",
    "        tf_idf = tf[word] * idf[word]\n",
    "        tf_idf_sentence.append(tf_idf)\n",
    "        tf_idf_sentence_with_sentence[word] = tf_idf \n",
    "\n",
    "    dict[kalimat]['TF-IDF'] = tf_idf_sentence\n",
    "    dict[kalimat]['TF-IDF with sentence'] = tf_idf_sentence_with_sentence\n",
    "\n",
    "    # print(tf_idf_sentence)\n",
    "    # all_tf_idf.append(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUM TF-IDF per kalimat\n",
    "for kalimat in dict:\n",
    "    tf_idf = dict[kalimat]['TF-IDF']\n",
    "    # print(len(tf_idf))\n",
    "    sum = 0\n",
    "    for n in range(len(tf_idf)):\n",
    "        sum += tf_idf[n]\n",
    "    dict[kalimat]['Sum TF-IDF'] = sum\n",
    "    # print('TF-IDF ' + kalimat + ' : ' + str(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tittle Similarity\n",
    "for kalimat in dict:\n",
    "    word_in_title = 0\n",
    "    sentence = dict[kalimat]['kalimat'].split()\n",
    "    for word in sentence:\n",
    "        for word_title in title_lower.split():\n",
    "            if word == word_title:\n",
    "                word_in_title += 1\n",
    "    title_similarity = word_in_title / len(title_lower.split())\n",
    "    dict[kalimat]['title_similarity'] = title_similarity\n",
    "    # print(dict[kalimat]['title_similarity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "# Sentence length feature (L)\n",
    "\n",
    "# Max Length of Text\n",
    "\n",
    "max_length = 0\n",
    "for kalimat in dict:\n",
    "    length_sentence = dict[kalimat]['kalimat'].split()\n",
    "    if max_length <= len(length_sentence):\n",
    "        max_length = len(length_sentence)\n",
    "print(max_length)    \n",
    "\n",
    "# Length feature each sentence\n",
    "\n",
    "for kalimat in dict:\n",
    "    count = 0\n",
    "    sentence = len(dict[kalimat]['kalimat'].split())\n",
    "\n",
    "    \n",
    "    dict[kalimat]['length'] = sentence / max_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location feature\n",
    "\n",
    "for kalimat in dict:\n",
    "    loc_s = kalimat[8:] \n",
    "    loc = 1 - (int(loc_s) / len(dict))\n",
    "    dict[kalimat]['Loc'] = loc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISCORE\n",
    "iscore_list = {}\n",
    "for kalimat in dict:\n",
    "    t = dict[kalimat]['title_similarity']\n",
    "    l = dict[kalimat]['length']\n",
    "    loc = dict[kalimat]['Loc']\n",
    "    sum_tf_idf = dict[kalimat]['Sum TF-IDF']\n",
    "    iscore = t + l + loc + sum_tf_idf\n",
    "\n",
    "    dict[kalimat]['ISCORE'] = iscore\n",
    "    iscore_list[kalimat] = iscore \n",
    "\n",
    "import json\n",
    "\n",
    "with open('ISCORE.json', 'w') as json_file:\n",
    "    json.dump(iscore_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity\n",
    "import math\n",
    "# # print(len(kalimat_1))\n",
    "# print(len(kalimat_2))\n",
    "# print(dict['kalimat 15'])\n",
    "\n",
    "    # kalimat_2 = dict['kalimat 2']['TF-IDF']\n",
    "for kalimat1 in dict:\n",
    "\n",
    "    kalimat_1 = dict[kalimat1]['TF-IDF']\n",
    "    # cosine_similarity = {} \n",
    "    dict[kalimat1]['cosine_similarity'] = {}\n",
    "    dict[kalimat1]['edge ' + kalimat1] = {}\n",
    "    for kalimat2 in (dict): \n",
    "        kalimat_2 = dict[kalimat2]['TF-IDF']\n",
    "        \n",
    "        \n",
    "\n",
    "        ab = [] # atas\n",
    "        a = [] # ||A||\n",
    "        b = [] # ||B||\n",
    "\n",
    "        if kalimat_2 != kalimat_1:\n",
    "        # Calculate Cosine\n",
    "            # cosine_similarity= {}\n",
    "            for x in range(len(kalimat_1)):\n",
    "                ab.append(kalimat_1[x] * kalimat_2[x]) # A * B\n",
    "                a.append(pow(kalimat_1[x],2)) # ||A||\n",
    "                b.append(pow(kalimat_2[x],2)) # ||B||\n",
    "            \n",
    "            sum_ab = 0 # init sum A * B\n",
    "            for y in (ab):\n",
    "               sum_ab += y \n",
    "                \n",
    "            sum_a = 0 # init sum ||A||\n",
    "            sum_b = 0 # init sum ||B||\n",
    "\n",
    "            for z in (a):\n",
    "                sum_a += z\n",
    "\n",
    "            for z in (b):\n",
    "                sum_b += z\n",
    "        \n",
    "            a_b = math.sqrt(sum_a) * math.sqrt(sum_b) # ||A|| * ||B||\n",
    "            cosine_similarity = sum_ab / a_b\n",
    "            dict[kalimat1]['cosine_similarity'][kalimat2] = cosine_similarity\n",
    "            dict[kalimat1]['edge ' + kalimat1][int(kalimat2[8:])] = cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUMP cosine_similarity\n",
    "\n",
    "cosine_similarity = {}\n",
    "for x in dict:\n",
    "    cosine_similarity[x] = dict[x]['cosine_similarity']\n",
    "# print(cosine_similarity)\n",
    "\n",
    "# Export JSON\n",
    "import json\n",
    "\n",
    "with open('cosine_similarity.json', 'w') as json_file:\n",
    "    json.dump(cosine_similarity, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (1, 11), (1, 18), (1, 19), (1, 20), (1, 22), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 10), (2, 11), (2, 12), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (2, 21), (2, 22), (2, 23), (3, 4), (3, 5), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11), (3, 12), (3, 14), (3, 15), (3, 16), (3, 17), (3, 18), (3, 19), (3, 20), (3, 21), (3, 22), (3, 23), (4, 5), (4, 6), (4, 8), (4, 9), (4, 10), (4, 14), (4, 15), (4, 16), (4, 18), (4, 20), (4, 21), (4, 22), (4, 23), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 15), (5, 17), (5, 21), (5, 22), (5, 23), (6, 8), (6, 9), (6, 10), (6, 12), (6, 14), (6, 15), (6, 16), (6, 17), (6, 20), (6, 22), (6, 23), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 14), (7, 15), (7, 17), (7, 21), (7, 22), (7, 23), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (8, 16), (8, 17), (8, 18), (8, 20), (8, 21), (8, 22), (8, 23), (9, 10), (9, 11), (9, 14), (9, 15), (9, 16), (9, 17), (9, 18), (9, 19), (9, 20), (9, 21), (9, 22), (9, 23), (10, 11), (10, 12), (10, 14), (10, 15), (10, 16), (10, 17), (10, 18), (10, 19), (10, 20), (10, 21), (10, 22), (10, 23), (11, 12), (11, 15), (11, 17), (11, 20), (11, 21), (11, 22), (11, 23), (12, 13), (12, 15), (12, 16), (12, 17), (12, 21), (12, 23), (13, 14), (13, 15), (13, 16), (14, 16), (14, 17), (14, 18), (14, 20), (14, 21), (14, 22), (14, 23), (15, 16), (15, 17), (15, 20), (15, 21), (15, 22), (15, 23), (16, 17), (16, 18), (16, 19), (16, 20), (16, 21), (16, 22), (17, 18), (17, 19), (17, 20), (17, 21), (17, 22), (17, 23), (18, 19), (18, 20), (18, 21), (18, 22), (18, 23), (19, 20), (19, 22), (20, 21), (20, 22), (20, 23), (21, 22), (21, 23)]\n",
      "\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "# Graf I\n",
    "# Edge\n",
    "\n",
    "for kalimat in dict:\n",
    "    edge_kalimat = []\n",
    "    kalimat1 = dict[kalimat]['cosine_similarity']\n",
    "    for key in kalimat1:\n",
    "        if (kalimat1[key] != 0):\n",
    "            tupple = (int(kalimat[8:]), int(key[8:]))\n",
    "            edge_kalimat.append(tupple)\n",
    "    dict[kalimat]['edge'] = edge_kalimat\n",
    "\n",
    "\n",
    "# print(edge)\n",
    "\n",
    "edge = []\n",
    "# Kumpulkan semua edge\n",
    "for kalimat in dict:\n",
    "    edge1 = dict[kalimat]['edge']\n",
    "    for e in edge1:\n",
    "        edge.append(e) \n",
    "                        \n",
    "# Hapus duplikat edge\n",
    "for e in edge:\n",
    "    a = e[0]\n",
    "    b = e[1]\n",
    "    for x in edge:\n",
    "        if e != x:\n",
    "            c = x[0]\n",
    "            d = x[1]\n",
    "            if a == d and b == c:\n",
    "                edge.remove(x)\n",
    "                    \n",
    "                        \n",
    "print(edge)\n",
    "print()\n",
    "print(len(edge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('edge.json', 'w') as json_file:\n",
    "    json.dump(edge, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# Graf II\n",
    "\n",
    "vertex = [] # Vertex\n",
    "\n",
    "\n",
    "for i in range(len(dict)):\n",
    "    vertex.append(i + 1)\n",
    "print(vertex)\n",
    "print(len(vertex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "particle = {}\n",
    "for i in range(1,26):\n",
    "    jalur = []\n",
    "    while len(jalur) != math.ceil(6): # <--- len(vertex)*2/10 atau int 4\n",
    "        r = random.choice(vertex)\n",
    "        if r not in jalur:\n",
    "            jalur.append(r)\n",
    "        jalur.sort()\n",
    "        \n",
    "    particle[i] = jalur\n",
    "\n",
    "# Export JSON\n",
    "import json\n",
    "\n",
    "with open('particle.json', 'w') as json_file:\n",
    "    json.dump(particle, json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# new_vertex = vertex[:14]\n",
    "\n",
    "# def network(pos):\n",
    "#     find_net = []\n",
    "#     for ed in edge:\n",
    "#         if ed[0] == pos:\n",
    "#             find_net.append(ed)\n",
    "#     return random.choice(find_net)\n",
    "\n",
    "# def create_particles():\n",
    "#     solution = []\n",
    "\n",
    "#     for i in range(len(vertex)):\n",
    "#         if not solution:\n",
    "#             x = network(random.choice(new_vertex))\n",
    "#             a = x[0]\n",
    "#             solution.append(a)\n",
    "#         elif solution[-1] != 14:\n",
    "#             t = i - 1\n",
    "#             x = network(solution[-1])\n",
    "#         else:\n",
    "#             break\n",
    "#         b = x[1]\n",
    "#         solution.append(b)\n",
    "        \n",
    "#     return solution\n",
    "\n",
    "# particle = {}\n",
    "# for p in range(1,6):\n",
    "#     par = create_particles()\n",
    "#     while len(par) < 5:\n",
    "#         par = create_particles()\n",
    "#     particle[p] = par\n",
    "#     # if par not in particle.values():\n",
    "#     #     particle[p] = par\n",
    "#     # else:\n",
    "#     #     while par not in particle.values():\n",
    "#     #         par = create_particles()\n",
    "#     #     particle[p] = par\n",
    "\n",
    "\n",
    "\n",
    "# # Export JSON\n",
    "# import json\n",
    "\n",
    "# with open('particle.json', 'w') as json_file:\n",
    "#     json.dump(particle, json_file)\n",
    "\n",
    "\n",
    "# # with open('res.json', 'w') as json_file:\n",
    "# #     json.dump(res, json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tes = []\n",
    "# while len(tes) < 4:\n",
    "#     i = random.choice(new_vertex)\n",
    "#     tes.append(i)\n",
    "# print(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # PSO\n",
    "\n",
    "# # initialize parameters\n",
    "# iterations = 100 \n",
    "# population = 25  \n",
    "# c1 = 1\n",
    "# c2 = 2\n",
    "# pbest = 0\n",
    "# gbest = 0\n",
    "# prev = 0\n",
    "# v = 0\n",
    "# # def update(c1, c2, pbest, gbest, prev, v):\n",
    "#     # velocity = v + c1 * random.random() * (pbest - prev) + c2 * random.random() * (gbest - prev)\n",
    "\n",
    "#     # pos = prev + velocity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(particle[1])\n",
    "\n",
    "# def fun_object(x, y):\n",
    "#     iscore = dict['kalimat ' + str(x)]['ISCORE']\n",
    "#     sim = cosine_similarity['kalimat ' + str(x)]['kalimat ' + str(y)]\n",
    "#     return iscore * sim\n",
    "\n",
    "# def update(c1, c2, pbest, gbest, prev, v):\n",
    "#     velocity = v + c1 * random.random() * (pbest - prev) + c2 * random.random() * (gbest - prev)\n",
    "\n",
    "#     pos = prev + velocity\n",
    "\n",
    "# c1 = 1\n",
    "# c2 = 2\n",
    "# pbest = 0\n",
    "# gbest = 0\n",
    "# prev = 0\n",
    "# v = 0\n",
    "\n",
    "\n",
    "# fitness = []\n",
    "# x = particle[1]\n",
    "# a = x[0][0]\n",
    "# b = x[0][1]\n",
    "# # print(a,b)\n",
    "# f = fun_object(a, b)\n",
    "# # print(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export JSON\n",
    "import json\n",
    "\n",
    "with open('dict.json', 'w') as json_file:\n",
    "    json.dump(dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in dict:\n",
    "#     print(dict[d]['Loc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3e10ef16274dd72e574b8fa73b58450b957d8421a2901baded3cca26fcf5dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
